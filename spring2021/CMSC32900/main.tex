%% LyX 2.3.6.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{color}
\usepackage{babel}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=page,colorlinks=true]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz}
%\usetikzlibrary{quantikz}
\usetikzlibrary{calc,matrix,fit}
\tikzset{pics/grid/.style={code={\tikzset{wonderich/.cd,#1}
    \def\pv##1{\pgfkeysvalueof{/tikz/wonderich/##1}}%
    \draw[thick] (0.1,0.1) grid (\pv{nx}+0.9,\pv{ny}+0.9);}},
pics/nodes/.style={code={\tikzset{wonderich/.cd,#1}
    \def\pv##1{\pgfkeysvalueof{/tikz/wonderich/##1}}%   
    \path  foreach \X in {1,...,\pv{nx}} {
      foreach \Y in {1,...,\pv{ny}} { 
      (\X,\Y)node[minimum size=\pv{r},style/.expanded=\pv{style},inner sep=0pt]{}} };
}},
wonderich/.cd,nx/.initial=4,ny/.initial=4,r/.initial=4pt,
    style/.initial={circle,fill}}
\usepackage[braket, qm]{qcircuit}

\newcommand{\inputgroupvv}[5]{\POS"#1,0"."#2,0"."#1,0"."#2,0"!C*+<#3>\frm{\{}, \POS"#1,0"."#2,0"."#1,0"."#2,0"*!C!<1.7em,#4>=<0em>{#5}}
    % Constructs an input group with label #5 and a grouping { from rows #1 to #2 with #3 and #4 controlling the spacing
\usepackage{pgfplots}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\begin{document}
\title{Tensor Networks for Simulating Quantum Circuits on FPGAs}
\author{Maksim Levental}
\maketitle
\begin{abstract}
Most research in quantum computing today is performed against simulations
of quantum computers rather than true quantum computers. Simulating
a quantum computer entails implementing all of the unitary operators
corresponding to the quantum gates as tensors. For high numbers of
qubits, performing tensor multiplications for these simulations becomes
quite expensive, since $N$-qubit gates correspond to $2^{N}$-dimensional
tensors. One way to accelerate such a simulation is to use field programmable
gate array (FPGA) hardware to efficiently compute the matrix multiplications.
Though FPGAs can efficiently perform tensor multiplications, they
are memory bound, having relatively small block random access memory.
One way to potentially reduce the memory footprint of a quantum computing
system is to represent it as a tensor network; tensor networks are
a formalism for representing compositions of tensors wherein economical
tensor contractions are readily identified. Thus we explore tensor
networks as a means to reducing the memory footprint of quantum computing
systems and broadly accelerating simulations of such systems.
\end{abstract}
\global\long\def\twovec#1#2{\begin{pmatrix}#1\\
 #2 
\end{pmatrix}}%

\global\long\def\C{\mathbb{C}}%

\global\long\def\R{\mathbb{R}}%

\global\long\def\bbra#1{\bra{#1}}%

\global\long\def\kket#1{\ket{#1}}%

\tableofcontents{}

\section{Introduction}

Quantum computing (QC) refers to the manipulation and exploitation
of properties of quantum mechanical (QM) systems to perform computation.
QM systems exhibit properties such as superposition and entanglement
and clever \textit{quantum algorithms} operate on these systems to
perform general computation. Unsurprisingly, the technique was intially
conceived of as a way to simulate physical systems themselves:
\begin{quotation}
``\ldots{} {[}N{]}ature isn't classical, dammit, and if you want to
make a simulation of nature, you'd better make it quantum mechanical,
and by golly it's a wonderful problem, because it doesn't look so
easy.''
\end{quotation}
This closing remark from the keynote at the 1\textsuperscript{st}
Physics of Computation Conference in 1981, delivered by the late Richard
Feynman \cite{feynman1982simulating}, succinctly, but accurately,
expresses that initial goal of quantum computing. Although modeling
and simulating physical systems on quantum computers remains a thriving
area of research we narrow our focus here to QC as it pertains to
solving general computational problems. Such problems include unstructured
search \cite{10.1145/237814.237866}, integer factorization \cite{365700},
combinatorial optimization \cite{farhi2014quantum}, and many others.
It is conjectured that some quantum algorithms enable quantum computers
to exceed the computational power of classical computers \cite{Zhong1460}. 

QC systems are composed of so-called quantum bits, or \textit{qubits},
that encode initial and intermediate states of computations. Transformations
between states are effected by time-reversible transforms, called
\textit{unitary} \textit{operators.} A formalism for representing
quantum computation is the \textit{quantum circuit} formalism, where
semenatically related collections of $N$ qubits are represented as
\textit{registers} and transformations are represented as \textit{gates},
connected to those registers by \textit{wires}, and applied in sequence.
As already mentioned, in hardware, quantum circuits correspond to
physical systems that readily exhibit quantum mechnical properties;
examples of physical qubits include transmons, ion traps and topological
quantum computers \cite{NAP25196}. Current state of the art QC systems
are termed Noisy Intermediate-Scale Quantum (NISQ) systems. Such systems
are characterized by moderate quantities of physical qubits (50-100)
but relatively few logical qubits (i.e. qubits robust to inteference
and noise). Due to these limitations (and, more broadly, the relative
scarcity of functioning QC systems), most research on quantum algorithms
is performed with the assistance of simulators of QC systems. Such
simulators perform simulations by representing $N$-qubit circuits
as $2^{N}$-dimensional complex vectors and transformations on those
vectors as $2^{N}$-dimensional complex matrix-vector multiplication.
Naturally, due to this exponential growth, naively executing such
simulations quickly become infeasible for $N>50$ qubits \cite{pednault2020paretoefficient},
both due to memory constraints and compute time. 

It's the case that matrices are a subclass of a more general mathematical
object called a \textit{tensor} and composition of matrices can be
expressed as \textit{tensor contraction}.\textit{ Tensor networks}
(TNs) are decompositions (i.e. factorizations) of very high-dimensional
tensors into networks (i.e. graphs) of low-dimensional tensors. TNs
have been successfullly employed in reducing the memory requirements
of simulations of QC systems \cite{pednault2020paretoefficient}.
The critical feature of tensor networks that make them useful for
QC is the potential to perform tensor contractions on the low-dimensional
tensors in an order such that, ultimately, the memory and compute
time requirements are lower than for the traditional representation.
Existing applications of TNs to quantum circuits focus primarily on
memory constraints on general purpose computers \cite{Fried_2018}
and distributed environments \cite{McCaskey_2018}. 

FPGAs are known to be performant for matrix multiplication uses cases
\cite{10.1145/3020078.3021740}. Though FPGAs typically run at lower
clock speeds (100-300MHz) than either conventional processors or even
graphics processors they, nonetheless, excel at latency constrained
computations owing to their fully ``synchronous'' nature (all modules
in the same \textit{clock domain} execute simultaneously). At first
glance FPGAs seem like a suitable platform for performant simulation
of quantum systems when runtime is of the essence. Unfortunately,
BRAM is one of the more limited resources on an FPGA and therefore
it becomes necessary to explore memory reduction strategies for simulations
(as well as runtime reduction strategies). Hence, we explore tensor
networks as a means of reducing the memory footprint of quantum circuits
with particular attention to dimensions of the designs space as they
pertain to deployment to FPGAs. 

The remainder of this report is organized as follows: section \ref{sec:Background}
covers the necessary background wherein subsection \ref{sec:Quantum-Circuits}
very briefly reviews quantum computation and quantum circuits (with
particular focus on aspects that will be relevant for tensor networks
and FPGAs), section \ref{sec:Tensor-Networks} defines tensors and
tensor networks fairly rigorously and discusses algorithms for identifying
optimal contraction orders, section \ref{sec:FPGAs} discusses the
constraints imposed by virtue of deploying to FPGA, section \ref{sec:Implementation}
describes our implementation of TNs on FPGAs, section \ref{sec:Evaluation}
reports our results on various random circuits, and section \ref{sec:Conclusion}
concludes with future research directions.

\section{Background\label{sec:Background}}

\subsection{Quantum Computing\label{sec:Quantum-Circuits}}

We very (very) quickly review quantum computing and quantum circuits
as they pertain to our project. For a much more pedagogically sound
introduction consult \cite{j2020quantum}. As already alluded to,
quantum computing exploits properties of quantum mechanical systems
in order to perform arbitrary computation. The fundamental unit of
quantum computation is a qubit, defined as two-dimensional quantum
system with state vector $\psi$ an element of a Hilbert space\footnote{A Hilbert space $H$ is a vector space augmented with an inner product
such that, with respect to the metric induced by that inner product,
all Cauchy sequences converge.} $H$:
\[
\psi:=\alpha\twovec 10+\beta\twovec 01\equiv\twovec{\alpha}{\beta}
\]
where $\alpha,\beta\in\mathbb{\C}$ and $\left|\alpha\right|^{2}+\left|\beta\right|^{2}=1$.
This exhibits the superposition property of the qubit\footnote{We say that the qubit is in a superposition of the basis vectors/states.}
in that the squares of the coefficients are the probabilities of measuring
the system in the corresponding basis state. Collections of qubits
have state vectors that represented by the \textit{tensor product
}of the individual states of each qubit; for example, two qubits $\psi,\phi$
have state vector
\[
\psi\otimes\phi:=\twovec{\alpha}{\beta}\otimes\twovec{\alpha'}{\beta'}\equiv\begin{pmatrix}\alpha\alpha'\\
\alpha\beta'\\
\beta\alpha'\\
\beta\beta'
\end{pmatrix}
\]
where the second $\otimes$ is the Kronecker product and $\alpha\alpha'$
indicates conventional complex multiplication. Note that the basis
relative to which $\psi\otimes\phi$ is represented is the standard
basis for $\C^{4}$ and thus we observe exponential growth in the
size of the representation of an $N$-qubit system. An alternative
notation for state vectors is Dirac notation; for example, for a single
qubit
\[
\ket{\psi}\equiv\alpha\kket 0+\beta\kket 1
\]
and a 2-qubit system
\begin{align*}
\ket{\psi}\otimes\ket{\phi} & \equiv\left(\alpha\kket 0+\beta\kket 1\right)\otimes\left(\alpha'\kket 0+\beta'\kket 1\right)\\
 & \equiv\alpha\alpha'\kket 0\otimes\kket 0+\alpha\beta'\kket 0\otimes\kket 1+\beta\alpha'\kket 1\otimes\kket 0+\beta\beta'\kket 1\otimes\kket 1\\
 & \equiv\alpha\alpha'\kket 0\kket 0+\alpha\beta'\kket 0\kket 1+\beta\alpha'\kket 1\kket 0+\beta\beta'\kket 1\kket 1\\
 & \equiv\alpha\alpha'\kket{00}+\alpha\beta'\kket{01}+\beta\alpha'\kket{10}+\beta\beta'\kket{11}\\
 & \equiv\alpha\alpha'\kket 0+\alpha\beta'\kket 1+\beta\alpha'\kket 2+\beta\beta'\kket 3
\end{align*}
where in the last line we've used the decimal representation for the
bit strings identifying the basis states. Of particular import for
QC are the \textit{entangled }or \textit{bell states}; they correspond
to multi-qubit states, such as 
\[
\ket{\xi}=\frac{1}{\sqrt{2}}\ket{00}+\frac{1}{\sqrt{2}}\ket{11}
\]
that cannot be ``factored'' into component states\footnote{$\xi$ cannot be factored because there is no solution to the set
of equations (for $\alpha,\alpha',\beta,\beta'$)
\[
\alpha\alpha'=\frac{1}{\sqrt{2}},\quad\alpha\beta'=0,\quad\beta\alpha'=0,\quad\beta\beta'=\frac{1}{\sqrt{2}}
\]
}. Then, naturally, changes in qubit states are represented as unitary\footnote{A matrix $U$ is unitary iff $UU^{\dagger}=U^{\dagger}U=I$, i.e.
it is its own Hermitian conjugate or more simply if it is ``self-inverse''.} matrices $U$; for example
\[
\psi'=U\psi=\begin{pmatrix}U_{00} & U_{01}\\
U_{10} & U_{11}
\end{pmatrix}\twovec{\alpha}{\beta}=\twovec{U_{00}\alpha+U_{01}\beta}{U_{10}\alpha+U_{11}\beta}
\]
Matrix representations of transformations on multi-qubit states are
constructed using the Kronecker product on the individual matrix representations;
for example

\[
U\otimes V:=\begin{pmatrix}U_{00}V & U_{01}V\\ U_{10}V & U_{11}V \end{pmatrix}:=\begin{pmatrix}
 U_{00} V_{00} & U_{00} V_{01} & U_{01} V_{00} & U_{01} V_{01} \\
 U_{00} V_{10} & U_{00} V_{11} & U_{01} V_{10} & U_{01} V_{11} \\
 U_{10} V_{00} & U_{10} V_{01} & U_{11} V_{00} & U_{11} V_{01} \\
 U_{10} V_{10} & U_{10} V_{11} & U_{11} V_{10} & U_{11} V_{11}
\end{pmatrix}
\]Here we see again an exponential growth in representation size as
a function of number of qubits. 

\begin{figure}
\begin{equation*}
    \psi\begin{cases}\qquad\qquad\quad
	\Qcircuit @C=1.0em @R=0.7em {
	 	\lstick{ q_0 := \ket{0} } & \gate{H} & \ctrl{1} & \ctrl{2} & \qw & \qw & \qw
		 \\
	 	\lstick{ q_1 := \ket{0}  } & \gate{H} & \targ & \qw & \gate{H} & \qw & \qw 
		 \\
	 	\lstick{ q_2 := \ket{0}  } & \gate{H} & \qw & \targ & \gate{H} & \qw & \qw
		 \\
	 	%\lstick{c:} & {/_{_{1}}} \cw & \cw & \cw & \cw & \cw & \cw\\
	 }
	\end{cases} 
	\Rightarrow\; \frac{\ket{000} + \ket{111}}{\sqrt{2}}
\end{equation*}
\centering{}\caption{Quantum Circuit representing 3-qubit $q_{0},q_{1},q_{2}$ entanglement
effected by application of successive Hadamard gates.\label{fig:Quantum-Circuit-representing-1}}
\end{figure}

As already alluded to, quantum circuits are a formalism for representing
quantum computation in general and algorithms designed for quantum
computers in particular. In the quantum circuit formalism qubit states
are represented by wires and unitary transformations are represented
by gates (see figure \ref{fig:Quantum-Circuit-representing-1}), much
like classical combinational logic circuits might be, though, whereas
combinational logic is ``memoryless''\footnote{The output of a combinational logic circuit at any time is only a
function of the elements of the circuit and its inputs.}, sequences of quantum gates specified by a quantum circuit do in
fact connote the evolution (in time) of the qubits. In addition quantum
gates, as opposed to classical gates, are necessarily reversible and
hence there are no quantum analogs to some classical gates such as
NOT and OR.

\subsection{Tensors and Tensor Networks\label{sec:Tensor-Networks}}

We quickly define tensors and tensor networks and then move on to
tensor network methods for simulating quantum circuits.

\subsubsection{Tensors}

One definition of a tensor\footnote{There are several more at varying levels of mathematical sophistication.
Chapter 14 of \cite{roman2007advanced} is the standard reference.
Ironically, it is this author's opinion that one should shy away from
physics oriented expositions on tensors.} $T$ is as an element of a tensor product space\footnote{The collection of tensor products of elements of the component spaces
quotiented by an equivalence relation.}:
\[
T\in\underbrace{V\otimes\cdots\otimes V}_{p{\text{ copies}}}\otimes\underbrace{V^{*}\otimes\cdots\otimes V^{*}}_{q{\text{ copies}}}
\]
where $V^{*}$ is dual\footnote{The dual space to a vector space $V$ is the vector $V^{*}$ consisting
of linear maps $f:V\rightarrow\R$. The dual basis of the dual space
consists of $f_{i}$ such that $f_{i}\left(\mathbf{e}_{i}\right)=\delta_{ij}$.
It is convention to write $f_{i}$ as $\mathbf{e}^{i}$ (note the
superscript index).} to $V$. Then $T$, in effect, acts a multilinear map
\[
{\displaystyle T:\underbrace{V^{*}\times\dots\times V^{*}}_{p{\text{ copies}}}\times\underbrace{V\times\dots\times V}_{q{\text{ copies}}}\rightarrow\R}
\]
by ``applying'' $p$ elements from $V$ to $p$ elements of $V^{*}$
and $q$ elements from $V^{*}$ to $q$ elements of $V$. Note the
swapping of the orders of $V,V^{*}$ in both the definitions and the
description. $T$'s coordinate basis representation

\begin{equation}
{\displaystyle T\equiv T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\mathbf{e}^{j_{1}}\otimes\cdots\otimes\mathbf{e}^{j_{q}}}\label{eq:tensor_coord}
\end{equation}
is determined by its evaluation on each set of bases

\[
{\displaystyle T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}:=T\left(\mathbf{e}^{i_{1}},\ldots,\mathbf{e}^{i_{p}},\mathbf{e}_{j_{1}},\ldots,\mathbf{e}_{j_{q}}\right)}
\]
The pair $\left(p,q\right)$ is called the \textit{type }or\textit{
valence} of \textbf{$T$} while $\left(p+q\right)$ is the \textit{order}
of the tensor. \textbf{Note that we do not use rank to mean either
of these things}\footnote{The \textit{rank} of a tensor is the minimum number of distinct basis
tensors necessary to define it; the tensor in eqn. (\ref{eq:tensor_coord})
is in fact a rank 1 tensor. The definition is a generalization of
the rank of a matrix (which, recalling, is the dimension of its column
space, i.e. number of basis elements). Despite this obvious, reasonable
definition for rank, one should be aware that almost all literature
in this area of research uses rank to mean order.}. Furthermore, eqn. (\ref{eq:tensor_coord}) in fact represents a
linear sum of basis elements, as it employs Einstein summation convention\footnote{Repeated indices in juxtapose position indicate summation $a_{i}b^{i}:=\sum_{i}a[i]b[i]$.}.
Note we make liberal use of summation convention in the following
but occasionally use explicit sums when it improves presentation (i.e.
when we would like to emphasize a particular contraction).

There are two important operations on tensors we need to define. Firstly,
we can form the tensor product $Z$ of two tensors $T,W$, of types
$\left(p,q\right),\left(r,s\right)$ respectively, to obtain a tensor
of type $\left(p+r,q+s\right)$:
\begin{align*}
Z & :=T\otimes W\\
 & \;=\left(T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes{\mathbf{e}}^{j_{1}}\otimes\cdots\otimes{\mathbf{e}}^{j_{q}}\right)\otimes\left(W_{l_{1}\dots l_{s}}^{k_{1}\dots k_{r}}\;\mathbf{e}_{k_{1}}\otimes\cdots\otimes\mathbf{e}_{k_{r}}\otimes{\mathbf{e}}^{l_{1}}\otimes\cdots\otimes{\mathbf{e}}^{l_{s}}\right)\\
 & \;=\left(T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}W_{l_{1}\dots l_{s}}^{k_{1}\dots k_{r}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\mathbf{e}_{k_{1}}\otimes\cdots\otimes\mathbf{e}_{k_{r}}\otimes{\mathbf{e}}^{j_{1}}\otimes\cdots\otimes{\mathbf{e}}^{j_{q}}\otimes{\mathbf{e}}^{l_{1}}\otimes\cdots\otimes{\mathbf{e}}^{l_{s}}\right)\\
 & :=Z_{j_{1}\dots j_{q+s}}^{i_{1}\dots i_{p+r}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{p+r}}\otimes{\mathbf{e}}^{j_{1}}\otimes\cdots\otimes{\mathbf{e}}^{j_{q+s}}
\end{align*}
\begin{comment}
The attentive reader should notice that the coordinate representation
of two tensors is 
\end{comment}
Despite it being obvious, its important to note that the tensor product
$Z$ produces a tensor of order $\left(p+r+q+s\right)$, i.e. higher
than either of the operands. On the contrary, \textit{tensor contraction}
reduces the order of a tensor. We define the contraction $Y$ of type
$\left(a,b\right)$ of a tensor $T$ to be the ``pairing'' of the
$a$th and $b$th bases: 
\begin{align*}
Y & :=T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{a-1}}\otimes\mathbf{e}_{i_{a+1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\left(\mathbf{e}_{i_{a}}\cdot\mathbf{e}^{j_{b}}\right)\otimes\mathbf{e}^{j_{1}}\otimes\cdots\otimes\mathbf{e}^{j_{b-1}}\otimes\mathbf{e}^{j_{b+1}}\otimes\cdots\otimes\mathbf{e}^{j_{q}}\\
 & \;=T_{j_{1}\dots j_{q}}^{i_{1}\dots i_{p}}\delta_{i_{a}}^{j_{b}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{a-1}}\otimes\mathbf{e}_{i_{a+1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\mathbf{e}^{j_{1}}\otimes\cdots\otimes\mathbf{e}^{j_{b-1}}\otimes\mathbf{e}^{j_{b+1}}\otimes\cdots\otimes\mathbf{e}^{j_{q}}\qquad\left(\text{since }\mathbf{e}_{i}\cdot\mathbf{e}^{j}=\delta_{i}^{j}\right)\\
 & \;=\sum_{j_{b}}T_{j_{1}\dots j_{b}\dots j_{q}}^{i_{1}\dots j_{b}\dots i_{p}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{a-1}}\otimes\mathbf{e}_{i_{a+1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\mathbf{e}^{j_{1}}\otimes\cdots\otimes\mathbf{e}^{j_{b-1}}\otimes\mathbf{e}^{j_{b+1}}\otimes\cdots\otimes\mathbf{e}^{j_{q}}\\
 & :=Y_{j_{1}\dots i_{b-1}i_{b+1}\dots j_{q}}^{i_{1}\dots i_{a-1}i_{a+1}\dots i_{p}}\;\mathbf{e}_{i_{1}}\otimes\cdots\otimes\mathbf{e}_{i_{a-1}}\otimes\mathbf{e}_{i_{a+1}}\otimes\cdots\otimes\mathbf{e}_{i_{p}}\otimes\mathbf{e}^{j_{1}}\otimes\cdots\otimes\mathbf{e}^{j_{b-1}}\otimes\mathbf{e}^{j_{b+1}}\otimes\cdots\otimes\mathbf{e}^{j_{q}}
\end{align*}
where $\left(\cdot\right)$ means inner product. Notice that the order
of $Y$ is $\left(p-1,q-1\right)$. Finally notice that we can omit
writing out bases and just manipulate coordinates. We shall do as
such when it simplifies presentation.

As mentioned in the introduction, matrices can be represented as tensors;
for example, the two dimensional $N\times N$ matrix $M$ is taken
to be a tensor of type $\left(1,1\right)$ with basis representation
\[
{\displaystyle M\equiv M_{j}^{i}\;\mathbf{e}_{i}\otimes\mathbf{e}^{j}}
\]
where upper indices correspond to the row index and lower indices
correspond to the column index of the conventional matrix representation
and both range from $1$ to $N$. The attentive reader will notice
that the coordinate representation of the tensor product for type
$\left(1,1\right)$ tensors is exactly the Kronecker product for matrices.
Similarly, tensor contraction for type $\left(1,1\right)$ tensors
is the familiar matrix trace:

\[
M_{j}^{i}\left(\mathbf{e}_{i}\cdot\mathbf{e}^{j}\right)=M_{j}^{i}\delta_{i}^{j}=\sum_{i=1}^{N}M_{i}^{i}
\]
More usefully, we can express matrix-vector multiplication in terms
of tensor contraction; let\textit{ 
\[
\mathbf{x}:=\twovec{x^{1}}{x^{2}}\equiv x^{1}\mathbf{e}_{1}+x^{2}\mathbf{e}_{2}\equiv x^{i}\mathbf{e}_{i}
\]
}where we switch to valence index notation in the column vector for
closer affinity with tensor notation. Then it must be the case that
\[
\mathbf{y}=M\mathbf{x}=\left(M_{j}^{i}\mathbf{e}_{i}\otimes\mathbf{e}^{j}\right)\left(x^{k}\mathbf{e}_{k}\right)=\left(M_{j}^{i}x^{k}\mathbf{e}_{i}\right)\left(\mathbf{e}^{j}\cdot\mathbf{e}_{k}\right)=M_{j}^{i}x^{k}\delta_{k}^{j}\mathbf{e}_{i}=M_{j}^{i}x^{j}\mathbf{e}_{i}
\]
Letting $y^{i}:=M_{j}^{i}x^{j}$ we recognize conventional matrix-vector
multiplication. Employing tensor contraction in this way extends to
matrix-matrix multiplication (and tensor composition more broadly);
for two type $\left(1,1\right)$ tensors $M,L$ we can form the type
$\left(1,1\right)$ tensor $Z$ corresponding to matrix product $M\cdot L$
of $N\times N$ by first taking the tensor product

\[
Z_{lj}^{ik}:=M_{l}^{i}L_{j}^{k}
\]
The attentive reader will notice that the coordinate representation
of two tensors is exactly the Kronecker product of two matrices. Then
contracting along the off diagonal
\begin{equation}
Z_{j}^{i}:=Z_{kj}^{ik}=M_{k}^{i}L_{j}^{k}\equiv\sum_{k=1}^{N}M_{k}^{i}L_{j}^{k}\label{eq:matrix_mult}
\end{equation}
One can confirm that this is indeed conventional matrix multipliation
of two $N\times N$ matrices. In general, stated simply, when contracting
indices of a tensor product, contraction can be understood to be a
sum over shared indices.

\subsubsection{Tensor Networks}

\begin{comment}
\begin{figure}
\centering{}    \begin{tikzpicture}
     \draw[dashed]  pic{grid}
     (0.3,4.7) node{$\mathbf{S}\boldsymbol{'}$};
     \clip (0.35,0.35) rectangle (4.65,4.65);
     \path[rotate around={45:(45:2.5)},scale={sqrt(2)},
        shift={(0,-0.75)},transform shape]  pic{grid={nx=3,ny=3}}
        pic[red]{nodes={r=4pt,nx=3,ny=3}};
    \end{tikzpicture}\caption{Projected Entangled Pair State (PEPS) for a 3 \texttimes{} 3 lattice
with open boundary conditions.\label{fig:Projected-Entangled-Pair}}
\end{figure}
\end{comment}

\begin{figure}
\begin{centering}
\subfloat[Contraction \label{fig:Tensor-networks-demonstrating}]{\begin{centering}
\includegraphics[width=0.75\textwidth]{tensor_net_contract}
\par\end{centering}
}
\par\end{centering}
\centering{}\smallskip{}
\subfloat[State vector representation\label{fig:State-vector-representation}]{\begin{centering}
\includegraphics[width=0.75\textwidth]{state_vector}
\par\end{centering}
}\smallskip{}
\subfloat[State vector factorization\label{fig:State-vector-factorization}]{\begin{centering}
\includegraphics[width=0.75\textwidth]{factorizations}
\par\end{centering}
}\caption{Tensor network diagrammatic contraction. \cite{evenbly2019tensortrace}}
\end{figure}

Tensor networks (TNs) are a way to factor tensors with large orders
into networks of tensors with lower orders; since the number of parameters
a tensor consists of is exponential in the order of the tensor, smaller
order tensors are much preferrable computationally. They were first
used to study ground states of one dimensional quantum many-body systems
\cite{PhysRevLett.69.2863} but have since been applied in other areas
(such as machine learning \cite{glasser2019expressive}). TNs lend
themselves to a diagrammatic representation which can be used to reason
about such factorizations (figure \ref{fig:Tensor-networks-demonstrating}).
We will primarily be interested in TNs as a means to factoring the
state-vector of an $N$-qubit system (see figure \ref{fig:State-vector-representation})
\begin{equation}
\ket{\psi}:=\sum_{i_{1}i_{2}\dots i_{N}}C^{i_{1}i_{2}\dots i_{N}}\ket{i_{1}}\ket{i_{2}}\cdots\ket{i_{N}}\label{eq:state_vector}
\end{equation}
for which its common to propose an ansatz factorizations: 
\begin{itemize}
\item \textbf{Matrix Product States (MPS)} \cite{Kl_mper_1993}, which yields
factorization
\[
C^{i_{1}i_{2}\dots i_{N}}\equiv A_{j_{1}}^{i_{1}}A_{j_{2}}^{i_{1}j_{1}}\cdots A_{j_{N-1}}^{i_{N-1}j_{N-2}}A^{i_{N}j_{N-1}}
\]
where $j$ are called \textit{bond indices. }If each index $i$ has
dimension $d$ (i.e. takes on values 1 to $d$) then $C$ is specified
by $d^{N}$ parameters and can always be represented by an MPS factorization
$Ndm^{2}$ parameters, where $m:=d^{N/2}$ is the bond dimension.
While for this naive representation $d^{N}<Ndm^{2}$, in practice
$m$ is fixed to some moderate size such that $d^{N}>Ndm^{2}$ and
the MPS factorization functions as an approximation.
\item \textbf{Projected Entangled Pair States (PEPS)} \cite{Verstraete:2004cf},
which is a generalization of MPS to higher spatial dimensions, i.e.
TNs that correspond to lattices of contractions of tensors, which
themselves represent pairwise entangled quantum systems. Naturally,
such a series of contractions doesn't lend itself to being expressed
in traditional notation and thus we observe the power of tensor network
diagrams (see PEPS in figure \ref{fig:State-vector-factorization}).
\item \textbf{Tree Tensor Networks (TTN) }\cite{PhysRevA.74.022320}, a
further generalization where tensors are entangled (and therefore
contracted) hierarchically.\textbf{ }In fact TTNs bear the closest
resemblance to quantum circuits.
\item \textbf{Multi-scale Entanglement Renormalization Ansatz (MERA) }\cite{PhysRevLett.99.220405},
a specific type of TTN where the tensors are alternatingly unitaries
and isometries\footnote{A tensor, seen as a multlinear map, that preserves distances under
the ambient distance metric.}.
\end{itemize}
%

\subsubsection{TNs for Simulating Quantum Circuits}

Factoring eqn. (\ref{eq:state_vector}) is only the first step to
successfully simulating a quantum circuit. By representing some final
state as a tensor as well, and contracting across all indices (called
\textit{contracting the network}), we can calculate the amplitude
for that particular state. Since tensor contraction is associative\footnote{This can be observed by noting that summing is an associative operation
(or by analog with matrix-matrix multiplication).}, the order in which tensors are actually contracted is a ``hyperparameter''
of TN methods; finding the optimal contraction order, with respect
to accuracy (assuming some approximation has been made in constructing
the factorization), compute time, and memory requirements is critical. 

In particular we focus on contraction orders for TTNs as they most
closely resemble quantum circuits. For a TTN consisting of $N$ tensors
(corresponding to $N$ gates) with maximum order $p$, worst case,
we can see that contraction time takes $O\left(N\exp\left(O\left(p\right)\right)\right)$
since, in general, contracting across all indices of a pair of tensors
is exponential in their orders\footnote{Consider contracting two $\left(1,1\right)$ tensors (as in eqn. (\ref{eq:matrix_mult})),
i.e. two order 2 tensors, which effectively is matrix multplication
followed by trace. The complexity of this contraction is then $O\left(N^{2+1}+N\right)\equiv O\left(\exp\left(2\log N\right)\left(1+N\right)\right)$
(where $N$ here is the characteristic dimension of the matrix). Assuming
the ranges of all tensor indices is the same (i.e. $N$ is constant
across all tensors), for example $N=2$ as in the case of matrices
derived of unitary transformations operating on single qubits, we
recover the stated complexity. }. Markov et al. \cite{10.1137/050644756} showed that there in fact
exists a contraction ordering which results in a contraction time
of $O\left(N^{O\left(1\right)}\exp\left(O\left(\operatorname{tw}\left(G^{L}\right)\right)\right)\right)$
where $G^{L}$ is the line graph\footnote{A \textit{line graph }captures edge adjacency; given a graph $G$,
$G^{L}$ is defined such that each edge of $G$ corresponds to a vertex
of $G^{L}$ and two vertices are are connected in $G^{L}$ if the
edges in $G$ that they correspond to are adjacent on the same vertex
(in $G$). } of the tensor network and $\operatorname{tw}\left(G^{L}\right)$
is the tree-width\footnote{A\textit{ tree decomposition} of a graph $G$ is a tree $T$ and a
mapping from the vertices of $G$ into ``bags'' that satisfy the
following properties
\begin{enumerate}
\item Each vertex must appear in at least one bag.
\item For each edge in $G$, at least one bag must contain both of the vertices
it is adjacent on.
\item All bags containing a given vertex in $G$ must be connected in $T$.
\end{enumerate}
The width $w$ of a tree decomposition is the cardinality of the largest
bag (minus one). Finally the \textit{tree-width }of $G$ is the minimum
width over all possible tree decompositions. Intuitively, a graph
has low tree-width if it can be constructed by joining small graphs
together into a tree.} of $G^{L}$. %
\begin{comment}
Thus, in general, we can simulate arbitrary quantum circuits by
\begin{enumerate}
\item Constructing the TTN (with graph $G$) that corresponds to the quantum
circuit.
\item Computing a tree-decomposition of $G^{L}$ with width $\operatorname{tw}\left(G^{L}\right)$.
\item Finding a contraction ordering\footnote{Markov et al. also show that one can recover a contraction order from
the tree-decomposition of $G^{L}$ in polynomial time.} for the TTN and fully contracting to a single, real, number (i.e.
order 0 tensor).
\end{enumerate}
Note that, in general, computing the tree-width of a graph is NP-hard
\cite{Arnborg87complexityof} and Markov et al.'s results rely on
generating \textit{any} tree-decomposition that has a tree-width that
suits their needs (i.e. overall runtime complexity).
\end{comment}
{} For quantum circuits consisting of many few qubit gates this technique
produces a much more (runtime) efficient evaluation of the circuit;
indeed Markov et al. further show that any TTN corresponding to a
quantum circuit with $N$ gates, where the number of gates that act
on any pair of qubits is bounded by $r$, has contraction time $O\left(N^{O\left(1\right)}\exp\left(O\left(r\right)\right)\right)$.

Markov et al.'s results are not tight; their construction finds some
tree-decomposition with the correct corresponding tensor contraction
order that suits their aim (overall runtime complexity of the translation
from quantum circuit to TTN and the ultimate contraction). In reality
there are often contraction orders that are much more space and runtime
efficient. Though, in general problem is NP-hard \cite{Arnborg87complexityof},
for particular TTNs (corresponding to circuits) there are heuristics,
such as non-adjacent contractions \cite{pednault2020paretoefficient},
that produce more efficient orders. Alternatively, randomized search
and Bayesian optimization can be used to identify efficient contraction
orders \cite{Gray_2021,Fried_2018}.

\subsection{FPGAs\label{sec:FPGAs}}

\begin{figure}
\centering{}\includegraphics[width=0.5\textwidth]{murra1-2998435}\caption{FPGA floorplan diagram \cite{9103284}.\label{fig:FPGA-floorplan-diagram}}
\end{figure}

A field-programmable gate array (FPGA) is a device designed to be
configured by a user into various arrangements of (classical) gates
and memory. FPGAs consist of arrays (hence the name) of configurable
logic blocks (CLBs), block ram (BRAM), and programmable busses that
connect CLBs and RAM into various topologies (see figure \ref{fig:FPGA-floorplan-diagram}).
The CLBs typically contain arithmetic units (such as adders, multipliers,
and accumulators) and lookup tables (LUTs), that can be programmed
to represent truth tables for many boolean functions. Using hardware
description languages (such as VHDL or Verilog) designers specify
modules and compose them into circuits (also known as a \textit{dataflows})
that perform arbitrary computation. These circuits then go through
a \textit{place and route} procedure before ultimately being instantiated
on the FPGA as \textit{processing elements} (PEs) and connections
between PEs.

While modules consisting purely of combinational logic compute their
outputs at the stated clock speed of the FPGA, inevitably I/O (i.e.
fetching data from memory) interleaved with such modules (otherwise
arranged into a pipeline architecture) creates pipeline stalls. Thus,
it's essential that FPGA designs are as compute bound as possible
(rather than I/O bound). In particular, we explore I/O minimal generalized
matrix multiplication (GEMM) \cite{10.1145/3373087.3375296} and other
\textit{systolic array} architectures \cite{10.1145/3431920.3439292,genc2019gemmini}.
A systolic architecture \cite{1653825} is a gridded, pipelined, array
of PEs that processes data as the data flows\footnote{The relationship to cardiovascular ``systolic'' is in association
with the flow of data into the array, akin to how blood flows through
the veins into the human heart.} through the array. Crucially, a systolic architecture propagates
partial results as well as input data through the pipeline (see figure
\ref{fig:systolic-array-2}). Systolic arrays are particularly suited
for I/O efficient matrix multiplication owing to the pipelining of
inputs (see figure \ref{fig:systolic-array}).

\begin{figure}
\centering{}\subfloat[Gemmini systolic array architecture. The processing elements (PEs)
are either of type Weight Stationary (WS) or Output Stationary (OS).
\cite{genc2019gemmini}.\label{fig:systolic-array-2}]{\centering{}\includegraphics[width=0.8\textwidth]{systolic_arch_detail}}\smallskip{}
\subfloat[Systolic array architecture implementing matrix multiplication. Input
matrices $A$ and $B$ stream by to produce output matrix $C$ via
successive MAC operations. Note that $C$ remains in the processing
elements (i.e. this is a diagram of an OS architecture). \cite{10.1007/978-3-030-05677-3_16}.\label{fig:systolic-array}]{\centering{}\includegraphics[width=0.8\textwidth]{systolic_array}}\caption{Systolic arrays}
\end{figure}

One remaining hurdle to simulating quantum compuations (i.e. carrying
out tensor contractions) on FPGAs is BRAM. The standard remediation
is to perform arithmetic with reduced precision\footnote{Germaine to this issue is the fact that arithmetic on FPGAs is typically
performed in fixed precision (via an integer representation), owing
to higher compute cost incurred for floating point arithmetic.}. There is evidence that suggests that simulations of quantum circuits,
of varying depths \cite{betelu2020limits}, are robust to reduced
precision computation as long as that loss of precision is uncorrelated
\cite{10.1145/3295500.3356155} i.e. insofar as it can be treated
as uncorrelated noise.

\section{Implementation\label{sec:Implementation}}

\begin{figure}
\begin{equation*}
	n\underbrace{\begin{cases}\qquad
    \Qcircuit @C=0.5em @R=1.0em @!R { 	 	\lstick{ {q}_{0} :  } & \gate{H} & \ctrl{1} & \gate{R_z} & \gate{R_x} & \qw & \ctrl{1} & \gate{R_z} & \gate{R_x} & \qw & \ctrl{1} & \gate{R_z} & \gate{R_x} & \qw & \ctrl{1} & \gate{R_z} & \gate{R_x} & \gate{H} & \qw & \qw & \qw && \cdots\\ 	 	\lstick{ {q}_{1} :  } & \gate{H} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \gate{H} & \qw & \qw && \cdots\\ 	 	\lstick{ {q}_{2} :  } & \gate{H} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \gate{H} & \qw & \qw && \cdots\\ 	 	\lstick{ {q}_{3} :  } & \gate{H} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \targ & \gate{R_z} & \ctrl{1} & \gate{R_x} & \gate{H} & \qw & \qw && \cdots\\ 	 	\lstick{ {q}_{4} :  } & \gate{H} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \ctrl{1} & \gate{R_z} & \control\qw & \gate{R_x} & \gate{H} & \qw & \qw && \cdots\\ 	 	\lstick{ {q}_{5} :  } & \gate{H} & \targ & \gate{R_z} & \gate{R_x} & \qw & \targ & \gate{R_z} & \gate{R_x} & \qw & \targ & \gate{R_z} & \gate{R_x} & \qw & \targ & \gate{R_z} & \gate{R_x} & \gate{H} & \qw & \qw & \qw && \cdots\\ 	 	\lstick{\vdots} & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots && \vdots \\ \\	 }
	\end{cases}}_{k}
\end{equation*}
\centering{}\caption{Test quantum circuit for $n$ qubits and rounds $k$.\label{fig:Quantum-Circuit-representing-1-1}}
\end{figure}

We use quimb \cite{Gray2018} to specify quantum circuit and generate
TNs therefrom. In particular we simulate circuits for $n=2,4,10$
qubits and \textit{rounds} $k=2,4,10$ where each consists round consists
of alternating qubit couplings according of the form in figure \ref{fig:Quantum-Circuit-representing-1-1}.
We also use CoTenGra \cite{Gray_2021} to find optimal tensor contraction
orders. For deploying circuits to FPGAs we use Chisel \cite{6241660}
as a HDL, by way of an adaptation of the Gemmini systolic array generator
\cite{genc2019gemmini}. Notably we experiment with using Gemmini
as an accelerator (i.e. fully parameterizing the weights/entries of
the matrices) and ``hardcoding'' certain gates/tensors and routing
data instead. The advantage of the latter approach over the former
is a reduction in loads from memory for the weights. The success of
the approach depends heavily on whether certain sequences of fixed
gates can actually be pipelined in this way. We hypothesize that this
might be the case for low-depth circuits/TNs.

\section{Evaluation\label{sec:Evaluation}}

\begin{figure}
\subfloat[Runtimes for optimal contraction strategy.\label{fig:Quantum-Circuit-representing-1-1-1-1-1}]{% This file was created by tikzplotlib v0.9.8.
\begin{tikzpicture}

\begin{axis}[
colorbar,
colorbar style={ylabel={}},
colormap/viridis,
point meta max=0.5678581,
point meta min=0.0073435,
tick align=outside,
tick pos=left,
x grid style={white!69.0196078431373!black},
xlabel={num qubits},
xmin=0.7, xmax=29.3,
xtick style={color=black},
y grid style={white!69.0196078431373!black},
ylabel={num rounds},
ymin=1.4, ymax=14.6,
ytick style={color=black}
]
\addplot [
  colormap/viridis,
  only marks,
  scatter,
  scatter src=explicit,
  scatter/@pre marker code/.append style={/tikz/mark size=\perpointmarksize},
  visualization depends on={\thisrow{sizedata} \as\perpointmarksize}
]
table [x=x, y=y, meta=colordata]{%
x  y  colordata  sizedata
2 8 0.014041600000000001 2.114138145400701
2 10 0.013769600000000002 2.0935615130194587
2 12 0.015611 2.2291558117850703
2 14 0.009201999999999998 1.7114577332389023
4 6 0.01370965 2.0889990739872544
4 8 0.009170599999999998 1.7085352329516268
4 10 0.009034999999999998 1.6958566630675331
4 12 0.009337900000000001 1.724049270234357
4 14 0.009318799999999999 1.7222851585522962
6 4 0.01380215 2.096034540648509
6 6 0.0093112 1.7215827056039197
6 8 0.048068999999999994 3.911628550740552
6 12 0.0470719 3.870846306875898
6 14 0.009306699999999998 1.7211666443859188
8 2 0.0098659 1.7721211883222492
8 4 0.009144900000000001 1.7061395248226765
8 6 0.0092672 1.717510226241004
8 8 0.0483214 3.9218846661065165
8 10 0.046570799999999996 3.850187793795009
8 12 0.0089567 1.6884922734742844
8 14 0.009395500000000001 1.7293584173443648
10 2 0.013566599999999998 2.0780719193283503
10 4 0.014333050000000002 2.1359661781420094
10 6 0.0468923 3.8634547591367197
10 8 0.046985 3.8672716483776264
10 10 0.0455763 3.808856372413943
12 2 0.0073435 1.52889131372726
12 4 0.5678580999999999 13.444509927087102
12 6 0.0464031 3.8432493387204345
12 10 0.046626799999999996 3.8525019663998057
12 12 0.0090037 1.692916631802345
12 14 0.0480242 3.909805319458707
14 2 0.013781550000000002 2.0944697686851965
14 4 0.0093156 1.7219894238158724
14 6 0.047868999999999995 3.9034825402109683
14 8 0.5628554 13.385157388388528
14 10 0.0454605 3.804014534785352
14 12 0.045375700000000005 3.8004649587267387
14 14 0.0474879 3.887913070544046
16 2 0.01375595 2.0925235670954616
16 4 0.047262700000000005 3.8786833665225426
16 6 0.04679610000000001 3.8594897674233173
16 8 0.0467371 3.857055999277226
16 10 0.046142 3.8324215280019067
16 14 0.047298200000000006 3.880139773087842
18 2 0.013555849999999998 2.077248437386472
18 4 0.0471931 3.8758264008673367
18 8 0.0088638 1.6797128234183019
18 12 0.04721299999999999 3.876643478112903
18 14 0.0458865 3.8217962520747375
20 2 0.0132719 2.0553775756397297
20 4 0.5555990999999999 13.298597154768487
20 6 0.0088702 1.680319122198953
20 8 0.046558 3.8496586447300656
20 10 0.0094773 1.736870255468047
20 14 0.0091107 1.702946235221377
22 2 0.0091275 1.7045156162800472
22 4 0.5510534999999999 13.244084599026822
22 6 0.046277 3.8380237887391058
22 8 0.009069299999999999 1.699072644346513
24 2 0.015064866666666664 2.189816429301928
24 4 0.0090818 1.7002431368318915
24 6 0.5599425 13.350476899514385
24 8 0.046919100000000005 3.8645586269127676
24 10 0.0475875 3.891988143452025
24 12 0.04551809999999999 3.8064236798210467
26 2 0.014012150000000001 2.111919949167156
26 4 0.0092823 1.7189089145512628
26 6 0.0092419 1.7151641720610815
26 8 0.046721599999999995 3.856416364751684
26 10 0.046856600000000005 3.861983818319208
26 12 0.047094899999999995 3.8717918666732336
28 2 0.009108799999999997 1.7027686546536236
28 4 0.009212499999999998 1.7124338896635312
28 6 0.0136726 2.0861744293889943
28 12 0.0092231 1.7134187787174855
};
\end{axis}

\end{tikzpicture}
\centering{}}

\hfill{}\subfloat[Runtimes for greedy contraction strategy.\label{fig:Quantum-Circuit-representing-1-1-1-1}]{% This file was created by tikzplotlib v0.9.8.
\begin{tikzpicture}

\begin{axis}[
colorbar,
colorbar style={ylabel={}},
colormap/viridis,
point meta max=0.0616001,
point meta min=0.0087104,
tick align=outside,
tick pos=left,
x grid style={white!69.0196078431373!black},
xlabel={num qubits},
xmin=0.7, xmax=29.3,
xtick style={color=black},
y grid style={white!69.0196078431373!black},
ylabel={num rounds},
ymin=1.4, ymax=14.6,
ytick style={color=black}
]
\addplot [
  colormap/viridis,
  only marks,
  scatter,
  scatter src=explicit,
  scatter/@pre marker code/.append style={/tikz/mark size=\perpointmarksize},
  visualization depends on={\thisrow{sizedata} \as\perpointmarksize}
]
table [x=x, y=y, meta=colordata]{%
x  y  colordata  sizedata
2 2 0.015495566666666665 2.22089893106823
2 4 0.013989350000000001 2.1102010345664257
2 6 0.013740950000000002 2.0913823731104646
2 8 0.013808049999999999 2.096482488340909
2 10 0.0137095 2.088987645879381
2 12 0.0139794 2.1094504551938837
2 14 0.013835 2.098527406385903
4 2 0.0090867 1.7007017500979562
4 4 0.05080520000000001 4.021417340881785
4 6 0.0464044 3.843303173368853
4 8 0.0466712 3.8543357871442563
4 10 0.048891500000000004 3.944952192404213
4 12 0.0481764 3.9159959653636998
4 14 0.047275000000000005 3.8791880425339924
6 2 0.0139827 2.109699420662121
6 4 0.0485813 3.932417586378714
6 6 0.046500400000000004 3.8472765733049066
6 8 0.0465955 3.851208680619218
6 10 0.0480405 3.9104687810047016
6 12 0.0478006 3.900692700728539
6 14 0.06160009999999999 4.4280831992985545
8 2 0.009261799999999999 1.7170097564827733
8 4 0.0466416 3.8531133369562194
8 6 0.04784 3.902299957080766
8 8 0.046665 3.8540797654909262
8 10 0.0459605 3.8248766678090567
8 12 0.04776849999999999 3.8993827457907235
8 14 0.0472837 3.8795449688524686
10 2 0.0132136 2.0508582379282427
10 4 0.047919100000000006 3.905524710846122
10 6 0.046026199999999996 3.8276094998670365
10 12 0.0462656 3.8375510250972282
12 2 0.009119799999999999 1.703796496069567
12 4 0.046374000000000005 3.8420440733920675
12 6 0.046118599999999996 3.831449636489532
12 8 0.0470455 3.8697606838743304
12 12 0.047656699999999996 3.894816908776978
14 2 0.01371665 2.089532316170988
14 4 0.0463552 3.841265212924884
14 6 0.0475169 3.8891000283878743
16 2 0.009189300000000001 1.710276304317144
16 4 0.046451 3.845232440714509
16 10 0.008710400000000002 1.665114540389126
18 2 0.01385195 2.099812522089427
18 4 0.04854970000000001 3.9311384459539434
18 6 0.047036699999999994 3.869398741853973
20 2 0.0087724 1.6710301150962794
20 4 0.047033599999999995 3.8692712314871307
20 6 0.0470217 3.8687817171776895
20 12 0.0098028 1.7664450606465132
22 2 0.013486149999999999 2.0719012697417627
22 4 0.0464897 3.846833907997403
22 6 0.0466608 3.853906321804491
24 2 0.0093269 1.723033510250917
24 4 0.04852820000000001 3.930267906734124
24 6 0.04739120000000001 3.883952558684679
26 2 0.013852749999999999 2.099873157081757
26 4 0.046646499999999994 3.8533157288071
26 6 0.0469076 3.864084990933142
26 8 0.0464272 3.8442472277198934
28 2 0.009089200000000002 1.700935688820042
28 6 0.047786 3.900096950228112
28 8 0.0466899 3.8551078785077557
};
\end{axis}

\end{tikzpicture}
\centering{}}\caption{Test quantum circuit for $n$ qubits and rounds $k$.\label{fig:Quantum-Circuit-representing-1-1-1}}
\end{figure}


\section{Conclusion\label{sec:Conclusion}}

TODO

\bibliographystyle{plain}
\phantomsection\addcontentsline{toc}{section}{\refname}\nocite{*}
\bibliography{biblio}

\end{document}
